**Apache Pig:**

**Definition:**
Apache Pig is a high-level platform and scripting language built on top of Apache Hadoop for processing and analyzing large datasets. It simplifies the development of complex data processing tasks by providing a scripting language called Pig Latin, which abstracts the complexities of MapReduce programming. Pig allows users to express data transformations using a set of simple and easy-to-understand operations, making it accessible to those without extensive programming or MapReduce knowledge.

**Key Characteristics:**

1. **Pig Latin Scripting Language:**
   - Users write Pig scripts in Pig Latin, which resembles a series of data transformations and operations. It abstracts the underlying MapReduce implementation details.

2. **Data Flow Model:**
   - Pig follows a data flow model where data is processed through a sequence of operations. This model simplifies the expression of complex data processing tasks.

3. **Extensibility:**
   - Users can extend Pig's functionality by implementing User Defined Functions (UDFs) in languages such as Java, Python, or Ruby.

4. **Optimization Opportunities:**
   - Pig optimizes the execution plan for data processing tasks, improving performance by minimizing the number of MapReduce jobs.

5. **Multi-Language Support:**
   - Pig integrates with multiple scripting and programming languages, allowing users to use the language they are most comfortable with for custom UDFs.

6. **Built-In Operators:**
   - Pig provides a rich set of built-in operators for common data operations, including filtering, grouping, sorting, and joining.

7. **Hadoop Ecosystem Integration:**
   - Pig seamlessly integrates with the Hadoop ecosystem, working on Hadoop Distributed File System (HDFS) and supporting data stored in various formats.

**Use Cases:**

- **ETL (Extract, Transform, Load) Pipelines:** Pig is commonly used for preprocessing and transforming large datasets before analysis or storage.
  
- **Data Cleaning and Enrichment:** Pig is suitable for cleaning, enriching, and preparing data for further analysis or reporting.

- **Log Processing:** Pig is used for analyzing and processing log data generated by applications.

- **Data Exploration:** Analysts and data scientists use Pig to explore and understand large datasets before more advanced analytics.
-----------------------------------------------------
**Pig Latin Basics:**

Pig Latin is the scripting language used in Apache Pig for expressing data transformations and operations. It provides a simple and high-level syntax for working with large datasets on Apache Hadoop. Below are some basic concepts and examples in Pig Latin:

### 1. **Load Data:**

To load data into Pig, you use the `LOAD` statement. This statement tells Pig where to get the data.

```pig
-- Loading data from a file into a relation named 'mydata'
mydata = LOAD 'input/data.txt' USING PigStorage(',') AS (name:chararray, age:int, city:chararray);
```

- `LOAD`: Loads data from a specified location.
- `PigStorage(',')`: Specifies the data format (in this case, comma-separated values).
- `(name:chararray, age:int, city:chararray)`: Defines the schema of the data.

### 2. **Filtering Data:**

To filter data based on a condition, you use the `FILTER` statement.

```pig
-- Filtering data where age is greater than 25
filtered_data = FILTER mydata BY age > 25;
```

- `FILTER`: Filters rows based on a specified condition.

### 3. **Grouping Data:**

Grouping is done using the `GROUP` statement.

```pig
-- Grouping data by city
grouped_data = GROUP mydata BY city;
```

- `GROUP`: Groups data based on a specified column.

### 4. **ForEach - Generate:**

The `FOREACH` statement is used to iterate over grouped data, and `GENERATE` is used to create new columns.

```pig
-- Calculating the average age for each city
avg_age_per_city = FOREACH grouped_data GENERATE group AS city, AVG(mydata.age) AS avg_age;
```

- `FOREACH`: Iterates over grouped data.
- `GENERATE`: Creates new columns or performs computations.

### 5. **Storing Data:**

To store the results, you use the `STORE` statement.

```pig
-- Storing the results in an output directory
STORE avg_age_per_city INTO 'output/avg_age_per_city' USING PigStorage(',');
```

- `STORE`: Saves the results to a specified location.

### 6. **Run Pig Script:**

Save the Pig Latin script in a file (e.g., `myscript.pig`) and execute it using the Pig interpreter:

```bash
pig -f myscript.pig
```
-------------------------------------------------------------------
In Apache Pig, data types represent the nature of values stored in a Pig relation. Pig supports both primitive and complex data types. Below are the commonly used data types in Pig:

### Primitive Data Types:

1. **Int (Integer):**
   - Represents a 32-bit signed integer.
   - Example: `int`

2. **Long:**
   - Represents a 64-bit signed integer.
   - Example: `long`

3. **Float:**
   - Represents a 32-bit floating-point number.
   - Example: `float`

4. **Double:**
   - Represents a 64-bit floating-point number.
   - Example: `double`

5. **Chararray:**
   - Represents character arrays or strings.
   - Example: `chararray`

6. **Bytearray:**
   - Represents binary data.
   - Example: `bytearray`

7. **Boolean:**
   - Represents boolean values (true or false).
   - Example: `boolean`

8. **Datetime:**
   - Represents date and time values.
   - Example: `datetime`

### Complex Data Types:

9. **Tuple:**
   - An ordered set of fields.
   - Example: `(1, 'John', 25)`

10. **Bag:**
    - An unordered set of tuples.
    - Example: `{(1, 'John'), (2, 'Jane')}`

11. **Map:**
    - A collection of key-value pairs.
    - Example: `[('name', 'John'), ('age', 25)]`

### Atomic Data Types:

12. **Atom:**
    - Represents a single scalar value (either primitive or complex).
    - Example: `1`, `'John'`, `(1, 'John', 25)`

### Null Data Type:

13. **Null:**
    - Represents a missing or undefined value.
    - Example: `null`

### Examples:

- **Declaration of Variables with Data Types:**
  ```pig
  -- Declaration of variables with data types
  myint: int;
  mytuple: (name: chararray, age: int);
  mybag: { (id: int, product: chararray) };
  ```

- **Loading Data with Schema:**
  ```pig
  -- Loading data with a schema
  mydata = LOAD 'input/data.txt' USING PigStorage(',') AS (name:chararray, age:int, city:chararray);
  ```

Pig's data types are essential for defining the structure of relations and ensuring proper handling of values during data processing. Understanding these data types is crucial when working with Pig Latin scripts to perform transformations on large-scale datasets.
---------------------------------------------------------
In Apache Pig, operations are performed on data using a set of operators in Pig Latin scripts. Here are examples of some common Pig operations:

### 1. **Diagnostic Operators:**

- **DUMP:**
  - Used for displaying the contents of a relation. Helpful for debugging.
  ```pig
  -- Displaying contents of 'mydata'
  DUMP mydata;
  ```

- **DESCRIBE:**
  - Displays the schema of a relation.
  ```pig
  -- Displaying schema of 'mydata'
  DESCRIBE mydata;
  ```

- **EXPLAIN:**
  - Shows the execution plan of a Pig Latin script.
  ```pig
  -- Displaying the execution plan of a script
  EXPLAIN myscript;
  ```

### 2. **Grouping and Joining:**

- **GROUP BY:**
  - Groups data based on a specified column.
  ```pig
  -- Grouping 'mydata' by 'city'
  grouped_data = GROUP mydata BY city;
  ```

- **JOIN:**
  - Combines two or more relations based on a common column.
  ```pig
  -- Joining 'data1' and 'data2' on 'id'
  joined_data = JOIN data1 BY id, data2 BY id;
  ```

### 3. **Combining and Splitting:**

- **UNION:**
  - Combines two or more relations with the same schema.
  ```pig
  -- Combining 'data1' and 'data2'
  combined_data = UNION data1, data2;
  ```

- **SPLIT:**
  - Splits a relation into multiple relations based on a condition.
  ```pig
  -- Splitting 'mydata' into two relations based on age
  SPLIT mydata INTO young_data IF age < 30, old_data IF age >= 30;
  ```

### 4. **Filtering:**

- **FILTER:**
  - Filters rows based on a specified condition.
  ```pig
  -- Filtering 'mydata' where age is greater than 25
  filtered_data = FILTER mydata BY age > 25;
  ```

### 5. **Sorting:**

- **ORDER BY:**
  - Sorts the result set based on one or more columns.
  ```pig
  -- Sorting 'mydata' by 'age' in descending order
  sorted_data = ORDER mydata BY age DESC;
  ```

These examples demonstrate the use of various Pig operations to perform tasks such as displaying data, describing schemas, grouping, joining, combining, splitting, filtering, and sorting. Pig's high-level abstractions make these operations relatively straightforward, allowing users to express complex data processing tasks in a concise manner.
--------------------------------------------------------------------------
Saving and executing a Pig script involves creating a Pig Latin script file and then running it using the Pig interpreter. Below are the steps to save and execute a Pig script:

### Step 1: Create a Pig Latin Script File

Create a new file using a text editor and save it with a `.pig` extension. For example, let's call it `myscript.pig`.

### Step 2: Write Your Pig Latin Script

Open the `myscript.pig` file in the text editor and write your Pig Latin script. Here's a simple example:

```pig
-- myscript.pig
-- Load data from a file
mydata = LOAD 'input/data.txt' USING PigStorage(',') AS (name:chararray, age:int, city:chararray);

-- Filter data for people with age greater than 25
filtered_data = FILTER mydata BY age > 25;

-- Group data by city
grouped_data = GROUP filtered_data BY city;

-- Calculate average age for each city
avg_age_per_city = FOREACH grouped_data GENERATE group AS city, AVG(filtered_data.age) AS avg_age;

-- Store the results
STORE avg_age_per_city INTO 'output/avg_age_per_city' USING PigStorage(',');
```

### Step 3: Save the Script

Save the `myscript.pig` file.

### Step 4: Execute the Pig Script

Run the Pig script using the Pig interpreter. Open a terminal and navigate to the directory where the script is saved. Then, execute the following command:

```bash
pig -f myscript.pig
```

This command tells Pig to run the script specified by the `-f` option.

### Additional Notes:

- Make sure to replace `'input/data.txt'` and `'output/avg_age_per_city'` with the actual paths to your input data file and the desired output location.

- Ensure that Hadoop and Pig are properly configured and running in your environment.

- Monitor the terminal for any errors or warnings that might occur during script execution.

Executing the Pig script this way allows you to automate and reproduce your data processing tasks easily. You can incorporate more complex operations into your script as needed for your specific use case.
----------------------------------------------------------------------------------
**1. Pig Latin Basic:**

   **Question:** What is Pig Latin, and how does it differ from traditional programming languages?

   **Answer:** Pig Latin is a high-level scripting language used in Apache Pig for processing and analyzing large datasets. It abstracts complex MapReduce programming into simpler, more human-readable scripts, making it easier to work with big data.

**2. Pig Data Types:**

   **Question:** Name some of the basic data types supported in Pig.

   **Answer:** Pig supports primitive data types such as int, long, float, double, chararray, and bytearray. Additionally, complex data types include tuple, bag, and map.

**3. Download the Data:**

   **Question:** How can you load data into Pig from an external source, and what are some supported data sources?

   **Answer:** Use the `LOAD` command to load data into Pig. Supported data sources include HDFS, HBase, and local files. For example: `LOAD 'input_data.txt' USING PigStorage(',') AS (col1:int, col2:chararray);`

**4. Create Your Script:**

   **Question:** Explain the structure of a Pig script. How do you define operations in Pig?

   **Answer:** A Pig script consists of a series of statements, where each statement represents a Pig operation. Operations are defined using keywords such as `LOAD`, `FILTER`, `GROUP`, etc. For example, a simple script may begin with loading data using `LOAD 'data.txt' AS (col1:int, col2:chararray);`.

**5. Save and Execute the Script:**

   **Question:** How do you save and execute a Pig script?

   **Answer:** Save the Pig script with a `.pig` extension. To execute, use the command `pig script_name.pig`. Alternatively, you can run Pig scripts in interactive mode using `pig -x local` for local execution.

**6. Pig Operations: Diagnostic Operators, Grouping and Joining, Combining & Splitting, Filtering, Sorting:**

   - **Question:** What is the purpose of Diagnostic Operators in Pig, and how are they useful during script development?

     **Answer:** Diagnostic Operators like `DUMP` and `DESCRIBE` are used for debugging and understanding the intermediate results of a Pig script. They help display data and schema information during development.

   - **Question:** How do you perform grouping and joining in Pig Latin?

     **Answer:** Grouping is done using the `GROUP` operator, while joining is accomplished with the `JOIN` operator. For example: `grouped_data = GROUP input_data BY col1;` and `joined_data = JOIN data1 BY key, data2 BY key;`.

   - **Question:** Explain the process of combining and splitting data in Pig.

     **Answer:** The `COGROUP` operator combines data based on a common field, while the `SPLIT` operator divides data into multiple datasets based on specified conditions.

   - **Question:** How can you filter data in Pig, and what operator is used for this purpose?

     **Answer:** Filtering is done using the `FILTER` operator. For example: `filtered_data = FILTER input_data BY col1 > 50;` to retain only records where the value of `col1` is greater than 50.

   - **Question:** Discuss the sorting operation in Pig and provide an example.

     **Answer:** Sorting is achieved using the `ORDER` operator. For instance: `sorted_data = ORDER input_data BY col1 DESC;` to sort the data in descending order based on the values in `col1`.
-----------------------------------------------------------------
**7. Pig UDFs (User-Defined Functions):**

   **Question:** What are Pig UDFs, and why might you need them in your Pig scripts?

   **Answer:** Pig UDFs are user-defined functions written in Java, Python, or other languages to perform custom processing within a Pig script. They are used when the built-in Pig functions are insufficient for specific data transformations or analysis.

**8. Parameter Substitution in Pig:**

   **Question:** How can you use parameter substitution in Pig scripts, and what is its significance?

   **Answer:** Parameter substitution allows the dynamic passing of values to Pig scripts. It is achieved using the `-param` option. For example: `pig -param input_file=data.txt -f script.pig`.

**9. Pig Execution Modes:**

   **Question:** Explain the difference between local and MapReduce execution modes in Pig.

   **Answer:** In local mode (`-x local`), Pig runs on a single machine and is suitable for small-scale testing. In MapReduce mode (default), Pig leverages the Hadoop cluster for distributed processing of large datasets.

**10. Pig Error Handling:**

   **Question:** How does Pig handle errors during script execution, and what are some common error-handling techniques?

   **Answer:** Pig aborts the entire script on encountering an error by default. Techniques for error handling include using the `DEFINE` statement for UDFs, using `DESCRIBE` to check the schema, and employing diagnostic operators like `ILLUSTRATE` for step-by-step debugging.

**11. Pig Replicated Join vs. Skewed Join:**

   - **Question:** Differentiate between replicated join and skewed join in Pig.

     **Answer:** In a replicated join, the smaller dataset is broadcasted to all nodes, while in a skewed join, the larger dataset is redistributed based on a specified key. Replicated join is suitable for small datasets, while skewed join handles imbalanced data distributions.

**12. Pig Sampling:**

   **Question:** How can you perform sampling in Pig, and why might you use this feature?

   **Answer:** Sampling is achieved using the `SAMPLE` operator. It is useful for extracting a subset of data for analysis or testing purposes. For example: `sampled_data = SAMPLE input_data 0.1;` to get a 10% sample of the data.

**13. Pig Streaming:**

   **Question:** What is Pig Streaming, and when might you use it in your data processing tasks?

   **Answer:** Pig Streaming allows the integration of non-Pig scripts or programs into Pig workflows. It is useful when tasks require languages or tools outside the scope of Pig, such as using Python or Perl scripts within a Pig script.

**14. Pig Multiquery:**

   **Question:** Explain the concept of multiquery in Pig and its advantages.

   **Answer:** Multiquery in Pig allows the execution of multiple queries in a single script. This can improve performance by avoiding redundant data reads. Use the `STORE` statement to save intermediate results for subsequent queries.

**15. Pig Execution Optimization:**

   **Question:** What are some optimization techniques you can employ to enhance the performance of Pig scripts?

   **Answer:** Techniques include using appropriate storage formats (like Parquet or ORC), choosing optimal join strategies, leveraging partitioning, and using built-in functions for efficient data processing.
  -----------------------------------------------------------------
  
  
Apache Pig is a high-level platform and scripting language built on top of Hadoop for processing and analyzing large datasets in a distributed computing environment. Pig simplifies the development of complex data processing tasks by providing a high-level scripting language called Pig Latin. Pig Latin abstracts the complexities of writing MapReduce programs, making it easier for users to express data transformations in a more intuitive manner. Here are key aspects of Apache Pig:

### 1. **Pig Latin:**
   - **Declarative Scripting Language:**
     - Pig Latin is a high-level, declarative scripting language specifically designed for data processing on Hadoop. It allows users to express the sequence of data transformations they want to perform without specifying how those transformations are implemented at the low level.
   - **Data Flow Language:**
     - Pig Latin is a data flow language, meaning that it focuses on the flow of data through a series of transformations. Users define a sequence of operations on their data, and Pig takes care of the underlying execution details.
   - **Readability and Expressiveness:**
     - Pig Latin scripts are often more readable and expressive than equivalent MapReduce programs, making it easier for users to understand, write, and maintain their data processing logic.

### 2. **Components of Apache Pig:**
   - **Pig Compiler:**
     - The Pig Compiler is responsible for compiling Pig Latin scripts into a series of MapReduce jobs that will be executed on a Hadoop cluster. It performs tasks such as parsing, optimization, and code generation.
   - **Execution Engine:**
     - The Execution Engine is responsible for executing the generated MapReduce jobs on the Hadoop cluster. It manages the flow of data between MapReduce jobs and handles the orchestration of the entire data processing workflow.

### 3. **Key Concepts in Pig:**
   - **Relation:**
     - A relation in Pig represents a bag of tuples. It is similar to a table in a relational database.
   - **Tuple:**
     - A tuple is an ordered set of fields. Fields can be of different data types.
   - **Bag:**
     - A bag is a collection of tuples. It is an unordered set of tuples.
   - **Map:**
     - A map is a collection of key-value pairs.
### 4. **Pig Latin Commands:**
   - **LOAD:**
     - The `LOAD` command is used to load data from a specified location (HDFS, local file system, etc.) into a Pig relation.
   - **FOREACH:**
     - The `FOREACH` command is used to apply transformations to each tuple in a relation.
   - **FILTER:**
     - The `FILTER` command is used to filter out tuples based on a specified condition.
   - **GROUP:**
     - The `GROUP` command is used to group tuples in a relation based on one or more keys.
   - **JOIN:**
     - The `JOIN` command is used to join two or more relations based on a common key.
   - **STORE:**
     - The `STORE` command is used to store the result of a Pig Latin script into a specified location (HDFS, local file system, etc.).

### 5. **User-Defined Functions (UDFs):**
   - **Built-in Functions:**
     - Pig provides a set of built-in functions for common data transformations, such as mathematical operations, string manipulations, and date functions.
   - **Custom UDFs:**
     - Users can write their own custom User-Defined Functions (UDFs) in Java, Python, or other languages. These UDFs can be integrated into Pig Latin scripts to perform specialized operations.

### 6. **Execution Modes:**
   - **Local Mode:**
     - Pig can be run in local mode for development and testing on a single machine. It simulates the Hadoop environment, allowing users to test their scripts without the need for a full Hadoop cluster.

   - **MapReduce Mode:**
     - In MapReduce mode, Pig scripts are executed on a Hadoop cluster using the MapReduce framework.

-----------------------------------------------------\
### 1. Pig Data Models
In Apache Pig, data models refer to the ways in which data is represented and structured within Pig Latin scripts. Pig Latin, the scripting language used in Apache Pig, supports various data models to accommodate different types of data and their structures. The primary data models in Pig Latin include:

1. **Atomic Data Types:**
   - These are basic, indivisible data types representing single values. Pig Latin supports several atomic data types, including:
     - **int:** Represents integer values.
     - **long:** Represents long integer values.
     - **float:** Represents floating-point values.
     - **double:** Represents double-precision floating-point values.
     - **chararray:** Represents character sequences (strings).
     - **bytearray:** Represents binary data.

   Example:
   ```pig
   -- Using atomic data types
   age: int;
   name: chararray;
   salary: double;
   ```

2. **Tuple:**
   - A tuple is an ordered set of fields. It is similar to a row in a table and can contain a mix of atomic and complex data types. Tuples are enclosed in parentheses `( )`.
   
   Example:
   ```pig
   -- Tuple with mixed data types
   person: (name: chararray, age: int, address: (city: chararray, zip: int));
   ```

3. **Bag:**
   - A bag is an unordered collection of tuples. It can hold duplicate tuples and is used to represent a set of records. Bags are enclosed in curly braces `{ }`.
   
   Example:
   ```pig
   -- Bag containing tuples
   employees: { (name: chararray, age: int), (name: chararray, age: int), ... };
   ```

4. **Map:**
   - A map is a key-value pair collection where keys and values can be of any data type. Maps are useful for representing associative arrays or dictionaries. Maps are enclosed in square brackets `[ ]`.
   
   Example:
   ```pig
   -- Map with string keys and integer values
   scores: [ 'Alice' : 90, 'Bob' : 85, 'Charlie' : 95 ];
   ```

These data models can be used in combination to represent complex data structures. For example, a tuple can contain fields that are bags or maps, creating a nested structure.

Example of a tuple with nested bags:
```pig
-- Tuple with nested bag
person: (name: chararray, addresses: { (city: chararray, zip: int), (city: chararray, zip: int) });
```

These data models allow Pig Latin to handle a wide variety of structured and semi-structured data, providing flexibility in expressing data transformations and analyses in a Hadoop environment.

--------------------------------------
ig Latin is a scripting language used in Apache Pig for expressing data transformation and analysis tasks. It provides a high-level abstraction over the complexities of writing low-level MapReduce programs. Pig Latin scripts are concise and easy to understand, making them suitable for users who may not have extensive programming experience. Here are some basic concepts and syntax elements of Pig Latin:

### 2. **Basic Commands:**

- **LOAD:**
  - Used to load data from a specified location (HDFS, local file system, etc.) into a relation.
  - Example: `mydata = LOAD '/path/to/data' USING PigStorage(',') AS (field1: int, field2: chararray, field3: float);`

- **DUMP:**
  - Displays the contents of a relation.
  - Example: `DUMP mydata;`

- **FOREACH:**
  - Applies transformations to each tuple in a relation.
  - Example: `result = FOREACH mydata GENERATE field1 * 2, field2;`

- **FILTER:**
  - Filters out tuples based on a specified condition.
  - Example: `filtered_data = FILTER mydata BY field1 > 10;`

- **GROUP:**
  - Groups tuples in a relation based on one or more keys.
  - Example: `grouped_data = GROUP mydata BY field2;`

- **JOIN:**
  - Joins two or more relations based on a common key.
  - Example: `joined_data = JOIN mydata BY field1, otherdata BY key;`

- **STORE:**
  - Stores the result of a Pig Latin script into a specified location (HDFS, local file system, etc.).
  - Example: `STORE result INTO '/path/to/output';`

### 3. **Data Types:**

- **Primitive Data Types:**
  - Pig Latin supports primitive data types such as int, long, float, double, chararray, and bytearray.

- **Complex Data Types:**
  - Pig Latin also supports complex data types like tuple, bag, and map.

### 4. **Comments:**

- **Single-Line Comment:**
  - Denoted by `--`. Anything after `--` on a line is treated as a comment.
  - Example: `-- This is a comment`

### 5. **Schema Definition:**

- **Defining Schema:**
  - When loading data, you can define the schema (field names and data types) using the `AS` clause.
  - Example: `mydata = LOAD '/path/to/data' USING PigStorage(',') AS (field1: int, field2: chararray, field3: float);`

### 6. **User-Defined Functions (UDFs):**

- **Built-in Functions:**
  - Pig provides a set of built-in functions for common data transformations (e.g., mathematical operations, string manipulations).

- **Custom UDFs:**
  - Users can define their own User-Defined Functions (UDFs) in languages like Java and use them in Pig Latin scripts.

### 7. **Parameters:**

- **Defining Parameters:**
  - Users can define parameters to make scripts more flexible.
  - Example: `DEFINE myfunction(param1, param2) ...;`

- **Using Parameters:**
  - Parameters can be used in Pig Latin scripts to provide values at runtime.
  - Example: `result = FILTER mydata BY field1 > $threshold;`

### 8. **Control Structures:**

- **IF-ELSE:**
  - Pig Latin supports basic control structures like `IF` and `ELSE`.
  - Example: 
    ```pig
    data = LOAD '/path/to/data' USING PigStorage(',') AS (field1: int, field2: chararray);
    filtered_data = FILTER data BY field1 > 10;
    result = (field1 > 20) ? 'High' : 'Low';
    ```

### 9. **Pig Latin Script Execution:**

- **Local Mode:**
  - Pig can be run in local mode for development and testing on a single machine.

  ```bash
  pig -x local script.pig
  ```

- **MapReduce Mode:**
  - In MapReduce mode, Pig scripts are executed on a Hadoop cluster.

  ```bash
  pig script.pig
  ```

### 10. **Script Structure:**

- **Script Structure:**
  - Pig Latin scripts follow a logical structure with LOAD, TRANSFORM, and STORE operations.

  ```pig
  -- Loading data
  data = LOAD '/path/to/data' USING PigStorage(',') AS (field1: int, field2: chararray);

  -- Transformation
  result = FOREACH data GENERATE field1 * 2, field2;

  -- Storing result
  STORE result INTO '/path/to/output';
  ```
--------------------------------------\
